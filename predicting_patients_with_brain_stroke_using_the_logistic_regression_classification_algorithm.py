# -*- coding: utf-8 -*-
"""Predicting patients with Brain stroke using the Logistic Regression classification Algorithm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1306uMZYPGTLSn_hlcz_6yLjBdO65hp0R
"""



"""Brain stroke prediction using the Logistic Regression classification Algorithm"""

#Import the dependencies
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
from sklearn.model_selection import train_test_split

"""Load the dataset"""

df= pd.read_csv("Brain_stroke full_data.csv")

df.head()

df.shape

"""Data Cleaning"""

#Check data for missing values
df.info()

df.isnull().sum()

#Change the datatype of age from float to int
df['age']= df['age'].astype('int')
print(df.dtypes)

#Describe the data
df.describe()

"""Visualize the data

Categorical features:
1. Gender
2. Age
3. Marital status(Ever married)
4. Residence type
5. smoking status
6. Work type

Numerical features:
1. Hypertension
2. heart_disease
3. avg_glucose_level
4. bmi

Defining the feature matrix x and response vector y
"""

#define x and y
feature_cols = ['age','hypertension','heart_disease','avg_glucose_level','bmi']
x= df[feature_cols]
y=df.stroke
x,y

#Split x and y into training and testing sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

#Import the logistic regression from scikitlearn module
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#instatiate the model
logreg=LogisticRegression(solver='lbfgs')
#fit the model with data
logreg.fit(x_train, y_train)

"""Predict the target variable"""

#predict the target variable for the given test dataset
predictions = logreg.predict(x_test)

"""Model performance and Accuracy measurements"""

#Classification accuracy
print("Logistic Regression Model Accuracy:", accuracy_score(y_test, predictions))

#Null accuracy: accuracy that could be achieved by always predicting the most frequent class

#examine the class distribution of the testing set(using a pandas series method)
y_test.value_counts()

#calculate the percentage of ones
y_test.mean()

#Calculate the percentage of zeros
1-y_test.mean()

#Comments: If we compare the null accuracy of 95% with the model accuracy of 95.5% above, our model seem to be very good.

#calculate the null accuracy(for binary classification problem coded as 0/1)
max(y_test.mean(), 1-y_test.mean())

"""Compare the Actual and Predicted response values"""

#print the first 30 true and predicted responses
print ('Actual:', y_test.values[0:30])
print ('pred:', predictions[0:30])

# Classiication accuracy appears to be the easisest to understand but it does not tell the underlying distribution of response values and it does not tell what types of erros the classifier model is making.

"""Confusion matrix"""

from sklearn import metrics
#First argument is true values while the second argument is the prediected.
print (metrics.confusion_matrix(y_test, predictions))

import plotly.graph_objs as go
import numpy as np
from sklearn.metrics import confusion_matrix

#Make the confusion matrix more visual with heatmap

import seaborn as sns
#set the font scale
sns.set(font_scale=1.5)
#create a confusion matrix
con_mat= confusion_matrix(y_test,predictions)
#plot it using seaborn
sns.heatmap(con_mat, annot=True,fmt=".1f",linewidth=.1)
plt.xlabel("predicted")
plt.ylabel("Actual");

#save confusion matrix and slice into four pieces
confusion= metrics.confusion_matrix(y_test, predictions)
TP = confusion[1,1]
TN = confusion[0,0]
FP = confusion[0,1]
FN = confusion[1,0]

#Using the confusion matrix to compute the classification metrics
#CLASSIFICATION ACCURACY
#How often is the classifier correct?
print ((TP + TN) / (float(TP +TN + FP + FN)))
print (metrics.accuracy_score(y_test, predictions))

#Classification Error: Overall, how often is the classifier incorrect?
#Misclassification rate
print ((FP + FN) / (float(TP +TN + FP + FN)))
print (1-metrics.accuracy_score(y_test, predictions))

#Sensitivity
#The sensitivity describes how sensitive the classifier is predicting positive values.
#This is also known as True positive Rate or "Recall"

print (TP/ (float(TP + FN)))
print (metrics.recall_score(y_test, predictions))

#Specificity: When the actual value is negative, how often is the prediction correct?
print (TN/ (float(TN + FP)))

#False positive Rate: When the actual value is negative, how often is the prediction incorrect?
print (FP/ (float(TN + FP)))

#Precision: How precise is the classifier when predicting positive instances?

print (metrics.precision_score(y_test, predictions))

#F1 score
print (metrics.f1_score(y_test, predictions))

#Combine the score metrics
from sklearn.metrics import classification_report
print(classification_report(y_test,predictions))

#print the first 10 predicted responses
logreg.predict(x_test)[0:10]

#print the first 10 predicted probalities of the data
logreg.predict_proba(x_test)[0:10, :]

logreg.predict_proba(x_test)[0:10, 1]
predictions_prob = logreg.predict_proba(x_test)[:, 1]

# Commented out IPython magic to ensure Python compatibility.
#Plot the histogram of the predicted probabilities
# %matplotlib inline
plt.rcParams['font.size'] = 14

#histogram
plt.hist(predictions_prob, bins=8)
plt.xlim(0,1)
plt.title('Histogram of predicted probabilities')
plt.xlabel('predicted probability of stroke')
plt.ylabel('Frequency')

"""Area Under the Curve(AUC)"""

from sklearn.metrics import roc_curve

#make predictions with probabilities
y_prob = logreg.predict_proba (x_test)

y_prob[:10], len(y_prob)

y_prob_positive = y_prob[:,1]
y_prob_positive[:10]

#Calculate fpr, tpr, thresholds
fpr, tpr, thresholds= roc_curve(predictions, y_prob_positive)

#check the false positive rates
fpr

#AUC is useful as a single number summary of classifier performance. It is useful when there is a high class imbalance.
from sklearn.metrics import roc_auc_score
    
print (metrics.roc_auc_score(y_test, y_prob_positive))

"""Saving and loading the trained model"""

#pickle
#Saving the existing model
import pickle

pickle.dump(logreg, open("lr1_logistic_Regression_model_stroke_pkl.sav","wb"))

#Load the saved model
loaded_pickle_model = pickle.load(open("lr1_logistic_Regression_model_stroke_pkl.sav","rb"))

#Make some predictions
loaded_pickle_model.predict(x_test)